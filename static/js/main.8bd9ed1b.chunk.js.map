{"version":3,"sources":["pages/prediction.jsx","images/small_batch32.png","images/small_deep.png","images/loss_overfitting.png","images/bias_variance.jpeg","images/learningRate_small.png","images/learningRate_normal.png","images/learningRate_high.png","pages/documentation.jsx","App.js","reportWebVitals.js","index.js"],"names":["PredictionPage","model","tensorData","inputMax","inputMin","labelMax","labelMin","userInputValueRef","useRef","predictionValueRef","unNormalize","xs","preds","unNormXs","mul","sub","add","unNormPreds","startModelCreationChain","getData","a","dataMediumJson","map","dataEntry","x","y","filter","convertToTensor","data","tf","shuffle","inputs","d","labels","inputTensor","length","labelTensor","max","min","div","createModel","dense","inputShape","units","useBias","trainModel","compile","optimizer","adam","loss","meanSquaredError","metrics","fit","batchSize","epochs","callbacks","tfvis","fitCallbacks","name","height","testModel","inputData","normalizationData","predict","reshape","dataSync","predictedPoints","Array","from","val","i","originalPoints","scatterplot","values","series","xLabel","yLabel","modelSummary","console","log","run","useEffect","classes","makeStyles","theme","root","flexGrow","title","color","AppBar","background","DokuButton","marginRight","content","marginLeft","marginTop","display","flexDirection","width","horizontalImages","funcButton","marginBottom","table","prediction","useStyles","toggle","createData","value","rows","position","className","Toolbar","Typography","variant","to","Button","onClick","current","normalizedInputs","parseInt","pred","predElement","TextField","id","label","type","inputRef","InputProps","readOnly","TableContainer","component","Paper","Table","aria-label","TableHead","TableRow","TableCell","align","TableBody","row","scope","contentDiv","LoginButton","DocumentationPage","src","small_deep","loss_overfitting","bias_variance","learningRate_small","learningRate_normal","learningRate_high","small_32","alt","App","exact","path","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"mvuBA4VeA,MAnUf,WAEI,IAEIC,EACAC,EACAC,EACAC,EACAC,EACAC,EAPEC,EAAoBC,iBAAO,GAC3BC,EAAqBD,mBAQ3B,SAASE,EAAYC,EAAIR,EAAUC,EAAUQ,EAAOP,EAAUC,GAQ1D,MAAO,CAACO,SAPSF,EACZG,IAAIX,EAASY,IAAIX,IACjBY,IAAIZ,GAKSa,YAHEL,EACfE,IAAIT,EAASU,IAAIT,IACjBU,IAAIV,IAQb,SAASY,IACL,IAAMC,EAAO,uCAAG,sBAAAC,EAAA,+EAKLC,EAAeC,KAAI,SAAAC,GAAS,MAAK,CACpCC,EAAGD,EAAUC,EACbC,EAAGF,EAAUE,MAEZC,QAAO,SAAAH,GAAS,OAAoB,MAAfA,EAAUC,GAA4B,MAAfD,EAAUE,MAT/C,2CAAH,qDAYb,SAASE,EAAgBC,GAIrB,OAAOC,KAAQ,WAEXA,IAAQC,QAAQF,GAGhB,IAAMG,EAASH,EAAKN,KAAI,SAAAU,GAAC,OAAIA,EAAER,KACzBS,EAASL,EAAKN,KAAI,SAAAU,GAAC,OAAIA,EAAEP,KAEzBS,EAAcL,IAAYE,EAAQ,CAACA,EAAOI,OAAQ,IAClDC,EAAcP,IAAYI,EAAQ,CAACA,EAAOE,OAAQ,IAWxD,OARAhC,EAAW+B,EAAYG,MACvBjC,EAAW8B,EAAYI,MACvBjC,EAAW+B,EAAYC,MACvB/B,EAAW8B,EAAYE,MAKhB,CACHP,OAJqBG,EAAYnB,IAAIX,GAAUmC,IAAIpC,EAASY,IAAIX,IAKhE6B,OAJqBG,EAAYrB,IAAIT,GAAUiC,IAAIlC,EAASU,IAAIT,IAMhEH,WACAC,WACAC,WACAC,eAKZ,IAAMkC,EAAc,WAShB,OARAvC,EAAQ4B,OAEFb,IAAIa,IAAUY,MAAM,CAACC,WAAY,CAAC,GAAIC,MAAO,EAAGC,SAAS,KAC/D3C,EAAMe,IAAIa,IAAUY,MAAM,CAACC,WAAY,CAAC,GAAIC,MAAO,EAAGC,SAAS,KAG/D3C,EAAMe,IAAIa,IAAUY,MAAM,CAACE,MAAO,EAAGC,SAAS,KAEvC3C,GAGL4C,EAAU,uCAAG,WAAO5C,EAAO8B,EAAQE,GAAtB,SAAAb,EAAA,6DAEfnB,EAAM6C,QAAQ,CAEVC,UAAWlB,IAASmB,OACpBC,KAAMpB,IAAUqB,iBAChBC,QAAS,CAAC,SAWI,GACH,GAlBA,SAoBFlD,EAAMmD,IAAIrB,EAAQE,EAAQ,CACnCoB,UAJc,GAKdC,OAJW,GAKXxB,SAAS,EACTyB,UAAWC,OAAWC,aAClB,CAACC,KAAM,wBACP,CAAC,OAAQ,OACT,CAACC,OAAQ,IAAKJ,UAAW,CAAC,kBA3BnB,mFAAH,0DAgCVK,EAAY,SAAC3D,EAAO4D,EAAWC,GAAuB,IACjD3D,EAA0C2D,EAA1C3D,SAAUC,EAAgC0D,EAAhC1D,SAAUE,EAAsBwD,EAAtBxD,SAAUD,EAAYyD,EAAZzD,SADkB,EAGnCwB,KAAQ,WACxB,IAAMlB,EAAKkB,IAAY,EAAG,EAAG,KACvBjB,EAAQX,EAAM8D,QAAQpD,EAAGqD,QAAQ,CAAC,IAAK,KAFf,EAGEtD,EAAYC,EAAIR,EAAUC,EAAUQ,EAAOP,EAAUC,GAA9EO,EAHuB,EAGvBA,SAAUI,EAHa,EAGbA,YAEjB,MAAO,CAACJ,EAASoD,WAAYhD,EAAYgD,eARU,mBAGhDtD,EAHgD,KAG5CC,EAH4C,KAWjDsD,EAAkBC,MAAMC,KAAKzD,GAAIW,KAAI,SAAC+C,EAAKC,GAC7C,MAAO,CAAC9C,EAAG6C,EAAK5C,EAAGb,EAAM0D,OAEvBC,EAAiBV,EAAUvC,KAAI,SAAAU,GAAC,MAAK,CACvCR,EAAGQ,EAAER,EAAGC,EAAGO,EAAEP,MAGjB+B,SAAagB,YACT,CAACd,KAAM,sCACP,CAACe,OAAQ,CAACF,EAAgBL,GAAkBQ,OAAQ,CAAC,WAAY,cACjE,CACIC,OAAQ,IACRC,OAAQ,IACRjB,OAAQ,QAKX,uCAAG,sCAAAvC,EAAA,sEAEWD,IAFX,cAEFS,EAFE,OAGF6C,EAAS7C,EAAKN,KAAI,SAAAC,GAAS,MAAK,CAClCC,EAAGD,EAAUC,EACbC,EAAGF,EAAUE,MAGjB+B,SAAagB,YACT,CAACd,KAAM,aACP,CAACe,UACD,CACIE,OAAQ,IACRC,OAAQ,IACRjB,OAAQ,MAIV1D,EAAQuC,IAlBN,SAmBFgB,OAAWqB,aAAa,CAACnB,KAAM,iBAAkBzD,GAnB/C,cAqBRC,EAAayB,EAAgBC,GACtBG,GAtBC,EAsBiB7B,GAAlB6B,OAAQE,EAtBP,EAsBOA,OAtBP,UAwBFY,EAAW5C,EAAO8B,EAAQE,GAxBxB,QAyBR6C,QAAQC,IAAI,iBAEZnB,EAAU3D,EAAO2B,EAAM1B,GA3Bf,4CAAH,qDA6BT8E,GA3JJC,qBAAU,WACN/D,OAmLJ,IAwCMgE,EAxCYC,aAAW,SAACC,GAAD,MAAY,CACrCC,KAAM,CACFC,SAAU,GAEdC,MAAO,CACHD,SAAU,EACVE,MAAO,SAEXC,OAAQ,CACJC,WAAY,WAEhBC,WAAY,CACRC,YAAa,IACbJ,MAAO,SAEXK,QAAS,CACLC,WAAY,IACZF,YAAa,IACbG,UAAW,IACXC,QAAS,OACTC,cAAe,SACfC,MAAO,MACPvC,OAAQ,SAEZwC,iBAAkB,CACdH,QAAS,OACTC,cAAe,OAEnBG,WAAY,CACRF,MAAO,cACPH,UAAW,GACXM,aAAc,IAElBC,MAAO,CACHJ,MAAO,KAEXK,WAAY,CACRT,WAAY,OAGJU,GAlPM,4CAoPtB,sBAAApF,EAAA,sDAEIoC,UAAciD,SAFlB,4CApPsB,sBAyPtB,SAASC,EAAWhD,EAAMiD,GACtB,MAAO,CAACjD,OAAMiD,SAGlB,IAAMC,EAAO,CACTF,EAAW,aAAc,IACzBA,EAAW,YAAa,QACxBA,EAAW,gBAAiB,MAC5BA,EAAW,SAAU,IACrBA,EAAW,OAAQ,OACnBA,EAAW,qBAAsB,MAGrC,OAAO,gCACH,cAACjB,EAAA,EAAD,CAAQoB,SAAS,SAASC,UAAW5B,EAAQO,OAA7C,SACI,eAACsB,EAAA,EAAD,WACI,cAACC,EAAA,EAAD,CAAYC,QAAQ,KAAKH,UAAW5B,EAAQK,MAA5C,8CAGA,cAAC,IAAD,CAAM2B,GAAG,iBAAT,SACI,cAACC,EAAA,EAAD,CAAQ3B,MAAM,UAAUsB,UAAW5B,EAAQS,WAA3C,kDAKZ,sBAAKmB,UAAW5B,EAAQW,QAAxB,UACI,cAACsB,EAAA,EAAD,CAAQF,QAAQ,YAAYzB,MAAM,UAAU4B,QA/FpD,WACI,IAAIT,EAAQpG,EAAkB8G,QAAQV,MADlB,EAE6BzG,EAA1CC,EAFa,EAEbA,SAAUC,EAFG,EAEHA,SAAUE,EAFP,EAEOA,SAAUD,EAFjB,EAEiBA,SAFjB,EAIDwB,KAAQ,WAEvB,IAAMyF,EAAmBzF,IAAU,CAAC0F,SAASZ,KAAS5F,IAAIX,GAAUmC,IAAIpC,EAASY,IAAIX,IAC/EO,EAAKkB,IAAY,EAAG,EAAG,KACzB2F,EAAOvH,EAAM8D,QAAQuD,EAAkB,CAAC,EAAG,IAJlB,EAKG5G,EAAYC,EAAIR,EAAUC,EAAUoH,EAAMnH,EAAUC,GAA7EO,EALsB,EAKtBA,SAAUI,EALY,EAKZA,YAEjB,MAAO,CAACJ,EAASoD,WAAYhD,EAAYgD,eAXzB,mBAchBwD,GAdgB,UAcG,IAEnBA,IACAhH,EAAmB4G,QAAQV,MAAQc,EACnC3C,QAAQC,IAAI0C,KA6EuDX,UAAW5B,EAAQkB,WAAtF,+BAEA,sBAAKU,UAAW5B,EAAQiB,iBAAxB,UACI,cAACuB,EAAA,EAAD,CAAWC,GAAG,YAAYC,MAAM,aAAaC,KAAK,SAASZ,QAAQ,WACxDa,SAAUvH,IACrB,cAACmH,EAAA,EAAD,CACIC,GAAG,aACHI,WAAY,CACRC,UAAU,GAEdlB,UAAW5B,EAAQqB,WACnBuB,SAAUrH,OAOlB,cAAC0G,EAAA,EAAD,CAAQF,QAAQ,UAAUzB,MAAM,UAAU4B,QArS5B,2CAqS4DN,UAAW5B,EAAQkB,WAA7F,+CAEA,cAACe,EAAA,EAAD,CAAQF,QAAQ,UAAWG,QAASlG,EAC5B4F,UAAW5B,EAAQkB,WAD3B,yCAIA,cAAC6B,EAAA,EAAD,CAAgBC,UAAWC,IAAOrB,UAAW5B,EAAQoB,MAArD,SACI,eAAC8B,EAAA,EAAD,CAAOC,aAAW,eAAlB,UACI,cAACC,EAAA,EAAD,UACI,eAACC,EAAA,EAAD,WACI,cAACC,EAAA,EAAD,qCACA,cAACA,EAAA,EAAD,CAAWC,MAAM,QAAjB,wBAGR,cAACC,EAAA,EAAD,UACK9B,EAAKtF,KAAI,SAACqH,GAAD,OACN,eAACJ,EAAA,EAAD,WACI,cAACC,EAAA,EAAD,CAAWN,UAAU,KAAKU,MAAM,MAAhC,SACKD,EAAIjF,OAET,cAAC8E,EAAA,EAAD,CAAWC,MAAM,QAAjB,SAA0BE,EAAIhC,UAJnBgC,EAAIjF,uBC9UhC,MAA0B,0CCA1B,MAA0B,uCCA1B,MAA0B,6CCA1B,MAA0B,2CCA1B,MAA0B,+CCA1B,MAA0B,gDCA1B,MAA0B,8CCiBnC8C,EAAYrB,aAAW,SAACC,GAAD,MAAY,CACrCC,KAAM,CACFC,SAAU,GAEduD,WAAY,CACRjD,YAAa,GACbE,WAAY,IAEhBP,MAAO,CACHD,SAAU,EACVE,MAAO,SAEXC,OAAQ,CACJC,WAAa,WAEjBoD,YAAa,CACTlD,YAAa,GACbJ,MAAO,SAEXW,iBAAkB,CACdH,QAAS,OACTC,cAAe,WAwLR8C,EApLW,WACtB,IAAM7D,EAAUsB,IAEhB,OACI,sBAAKM,UAAW5B,EAAQG,KAAxB,UACI,cAACI,EAAA,EAAD,CAAQoB,SAAS,SAASC,UAAW5B,EAAQO,OAA7C,SACI,eAACsB,EAAA,EAAD,WAEI,cAACC,EAAA,EAAD,CAAYC,QAAQ,KAAKH,UAAW5B,EAAQK,MAA5C,8CAGA,cAAC,IAAD,CAAM2B,GAAG,IAAT,SACI,cAACC,EAAA,EAAD,CAAQ3B,MAAM,UAAUsB,UAAW5B,EAAQ4D,YAA3C,2CAKZ,sBAAKhC,UAAW5B,EAAQ2D,WAAxB,UACI,6EAEA,4CAEA,0CALJ,yKASI,+CATJ,sJAYI,qDAZJ,wIAcI,6CAdJ,0EAiBI,uCACA,obAWA,wDACA,wMAKA,8CACA,sSAI4D,uBACxD,qBAAKG,IAAKC,IAAa,uBAL3B,6GAQA,6CACA,6WAKyD,uBACrD,qBAAKD,IAAKE,OAId,kDAvDJ,q8DAqFI,uBArFJ,gEAwFI,qBAAKF,IAAKG,IACV,uBAzFJ,0FAyFgG,uBAzFhG,IAyFsG,uBAzFtG,8MA8FI,wDACA,wDA/FJ,+hBA0GI,sBAAKrC,UAAU,mBAAf,UACI,qBAAKkC,IAAKI,IACV,qBAAKJ,IAAKK,IACV,qBAAKL,IAAKM,OAGd,uBAhHJ,ikBA2HI,2CA3HJ,mGA+HI,uBA/HJ,wXAwII,+DACA,4lCAmBA,qBAAKN,IAAKO,EAAUC,IAAKD,WCpM1BE,MAVf,WAEI,OAAO,qBAAK3C,UAAU,MAAf,SACH,eAAC,IAAD,WACI,cAAC,IAAD,CAAO4C,OAAK,EAACC,KAAK,IAAIzB,UAAWlI,IACjC,cAAC,IAAD,CAAO0J,OAAK,EAACC,KAAK,iBAAiBzB,UAAWa,UCF3Ca,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.8bd9ed1b.chunk.js","sourcesContent":["import React, {useEffect, useRef} from \"react\";\n\nimport TextField from '@material-ui/core/TextField';\nimport Button from '@material-ui/core/Button';\n\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as tfvis from \"@tensorflow/tfjs-vis\";\n\nimport dataSmallJson from \"../data/data_small.json\"\nimport dataMediumJson from \"../data/data_medium.json\"\nimport dataBigJson from \"../data/data_big.json\"\n\nimport Table from '@material-ui/core/Table';\nimport TableBody from '@material-ui/core/TableBody';\nimport TableCell from '@material-ui/core/TableCell';\nimport TableContainer from '@material-ui/core/TableContainer';\nimport TableHead from '@material-ui/core/TableHead';\nimport TableRow from '@material-ui/core/TableRow';\nimport Paper from '@material-ui/core/Paper';\nimport Toolbar from \"@material-ui/core/Toolbar\";\nimport Typography from \"@material-ui/core/Typography\";\nimport {Link} from \"react-router-dom\";\nimport AppBar from \"@material-ui/core/AppBar\";\nimport {makeStyles} from \"@material-ui/core/styles\";\n\nfunction PredictionPage() {\n\n    const userInputValueRef = useRef(0)\n    const predictionValueRef = useRef()\n    let model;\n    let tensorData;\n    let inputMax;\n    let inputMin;\n    let labelMax;\n    let labelMin;\n\n    function unNormalize(xs, inputMax, inputMin, preds, labelMax, labelMin) {\n        const unNormXs = xs\n            .mul(inputMax.sub(inputMin))\n            .add(inputMin);\n\n        const unNormPreds = preds\n            .mul(labelMax.sub(labelMin))\n            .add(labelMin);\n        return {unNormXs, unNormPreds};\n    }\n\n    useEffect(() => {\n        startModelCreationChain()\n    })\n\n    function startModelCreationChain() {\n        const getData = async () => {\n\n            // dataSmallJson\n            // dataMediumJson\n            // dataBigJson\n            return dataMediumJson.map(dataEntry => ({\n                x: dataEntry.x,\n                y: dataEntry.y,\n            }))\n                .filter(dataEntry => (dataEntry.x != null && dataEntry.y != null));\n        }\n\n        function convertToTensor(data) {\n            // Wrapping these calculations in a tidy will dispose any\n            // intermediate tensors.\n\n            return tf.tidy(() => {\n                // Step 1. Shuffle the data\n                tf.util.shuffle(data);\n\n                // Step 2. Convert data to Tensor\n                const inputs = data.map(d => d.x)\n                const labels = data.map(d => d.y);\n\n                const inputTensor = tf.tensor2d(inputs, [inputs.length, 1]);\n                const labelTensor = tf.tensor2d(labels, [labels.length, 1]);\n\n                //Step 3. Normalize the data to the range 0 - 1 using min-max scaling\n                inputMax = inputTensor.max();\n                inputMin = inputTensor.min();\n                labelMax = labelTensor.max();\n                labelMin = labelTensor.min();\n\n                const normalizedInputs = inputTensor.sub(inputMin).div(inputMax.sub(inputMin));\n                const normalizedLabels = labelTensor.sub(labelMin).div(labelMax.sub(labelMin));\n\n                return {\n                    inputs: normalizedInputs,\n                    labels: normalizedLabels,\n                    // Return the min/max bounds so we can use them later.\n                    inputMax,\n                    inputMin,\n                    labelMax,\n                    labelMin,\n                }\n            });\n        }\n\n        const createModel = () => {\n            model = tf.sequential();\n\n            model.add(tf.layers.dense({inputShape: [1], units: 1, useBias: true}));\n            model.add(tf.layers.dense({inputShape: [1], units: 1, useBias: true}));\n\n            // Add an output layer\n            model.add(tf.layers.dense({units: 1, useBias: true}));\n\n            return model;\n        }\n\n        const trainModel = async (model, inputs, labels) => {\n            // Prepare the model for training.\n            model.compile({\n                // optimizer: tf.train.adam(0.0001),\n                optimizer: tf.train.adam(),\n                loss: tf.losses.meanSquaredError,\n                metrics: ['mse'],\n            });\n\n            // adadelta\n            // adagrad\n            // adam\n            // adamax\n            // momentum\n            // rmsprop\n            // sgd\n\n            const batchSize = 64; // 5, 20, 32, 64\n            const epochs = 20;\n\n            return await model.fit(inputs, labels, {\n                batchSize,\n                epochs,\n                shuffle: true,\n                callbacks: tfvis.show.fitCallbacks(\n                    {name: 'Training Performance'},\n                    ['loss', 'mse'],\n                    {height: 200, callbacks: ['onEpochEnd']}\n                )\n            });\n        }\n\n        const testModel = (model, inputData, normalizationData) => {\n            const {inputMax, inputMin, labelMin, labelMax} = normalizationData;\n\n            const [xs, preds] = tf.tidy(() => {\n                const xs = tf.linspace(0, 1, 100);\n                const preds = model.predict(xs.reshape([100, 1]));\n                const {unNormXs, unNormPreds} = unNormalize(xs, inputMax, inputMin, preds, labelMax, labelMin);\n\n                return [unNormXs.dataSync(), unNormPreds.dataSync()];\n            });\n\n            const predictedPoints = Array.from(xs).map((val, i) => {\n                return {x: val, y: preds[i]}\n            });\n            const originalPoints = inputData.map(d => ({\n                x: d.x, y: d.y,\n            }));\n\n            tfvis.render.scatterplot(\n                {name: 'Model Predictions vs Original Data'},\n                {values: [originalPoints, predictedPoints], series: ['original', 'predicted']},\n                {\n                    xLabel: 'x',\n                    yLabel: 'y',\n                    height: 300\n                }\n            );\n        }\n\n        const run = async () => {\n            // Load and plot the original input data that we are going to train on.\n            const data = await getData();\n            const values = data.map(dataEntry => ({\n                x: dataEntry.x,\n                y: dataEntry.y,\n            }));\n\n            tfvis.render.scatterplot(\n                {name: 'x v value'},\n                {values},\n                {\n                    xLabel: 'x',\n                    yLabel: 'y',\n                    height: 300\n                }\n            );\n\n            const model = createModel();\n            await tfvis.show.modelSummary({name: 'Model Summary'}, model);\n\n            tensorData = convertToTensor(data);\n            const {inputs, labels} = tensorData;\n\n            await trainModel(model, inputs, labels);\n            console.log('Done Training');\n\n            testModel(model, data, tensorData);\n        }\n        run();\n    }\n\n    function predictValue() {\n        let value = userInputValueRef.current.value;\n        const {inputMax, inputMin, labelMin, labelMax} = tensorData;\n\n        const [xs, pred] = tf.tidy(() => {\n\n            const normalizedInputs = tf.tensor([parseInt(value)]).sub(inputMin).div(inputMax.sub(inputMin));\n            const xs = tf.linspace(0, 1, 100);\n            var pred = model.predict(normalizedInputs, [1, 1]);\n            const {unNormXs, unNormPreds} = unNormalize(xs, inputMax, inputMin, pred, labelMax, labelMin);\n\n            return [unNormXs.dataSync(), unNormPreds.dataSync()];\n        });\n\n        let predElement = pred[0];\n\n        if (predElement) {\n            predictionValueRef.current.value = predElement\n            console.log(predElement);\n        }\n    }\n\n    const useStyles = makeStyles((theme) => ({\n        root: {\n            flexGrow: 1,\n        },\n        title: {\n            flexGrow: 1,\n            color: \"white\"\n        },\n        AppBar: {\n            background: \"#607d8b\"\n        },\n        DokuButton: {\n            marginRight: 750,\n            color: \"white\"\n        },\n        content: {\n            marginLeft: 100,\n            marginRight: 100,\n            marginTop: 100,\n            display: \"flex\",\n            flexDirection: \"column\",\n            width: \"400\",\n            height: \"100vh\"\n        },\n        horizontalImages: {\n            display: \"flex\",\n            flexDirection: \"row\"\n        },\n        funcButton: {\n            width: \"fit-content\",\n            marginTop: 10,\n            marginBottom: 10\n        },\n        table: {\n            width: 250,\n        },\n        prediction: {\n            marginLeft: 20\n        }\n    }));\n    const classes = useStyles();\n\n    async function toggleVisorVisibility() {\n        // await model.save('downloads://my-model')\n        tfvis.visor().toggle()\n    }\n\n    function createData(name, value) {\n        return {name, value};\n    }\n\n    const rows = [\n        createData('Batch Size', 32),\n        createData('Optimizer', \"Adam\"),\n        createData('Learning Rate', 0.001),\n        createData('Epochs', 20),\n        createData('Loss', \"MSE\"),\n        createData('Training Data size', 500)\n    ];\n\n    return <div>\n        <AppBar position=\"static\" className={classes.AppBar}>\n            <Toolbar>\n                <Typography variant=\"h6\" className={classes.title}>\n                    Deep Learning - André Schwarz\n                </Typography>\n                <Link to=\"/documentation\">\n                    <Button color=\"primary\" className={classes.DokuButton}>Aufgabe 3 - Zur Dokumentation</Button>\n                </Link>\n            </Toolbar>\n        </AppBar>\n\n        <div className={classes.content}>\n            <Button variant=\"contained\" color=\"primary\" onClick={predictValue} className={classes.funcButton}> Wert\n                vorhersagen</Button>\n            <div className={classes.horizontalImages}>\n                <TextField id=\"userInput\" label=\"User Input\" type=\"number\" variant=\"outlined\"\n                           inputRef={userInputValueRef}/>\n                <TextField\n                    id=\"prediction\"\n                    InputProps={{\n                        readOnly: true,\n                    }}\n                    className={classes.prediction}\n                    inputRef={predictionValueRef}\n                />\n                {/*<TextField id=\"prediction\" label=\"Prediction\" type=\"readOnly\" variant=\"outlined\" InputProps={{*/}\n                {/*    readOnly: true,*/}\n                {/*}}*/}\n                {/*           inputRef={predictionValueRef}/>*/}\n            </div>\n            <Button variant=\"primary\" color=\"primary\" onClick={toggleVisorVisibility} className={classes.funcButton}>Modelldetails\n                anzeigen/ausblenden</Button>\n            <Button variant=\"primary\"  onClick={startModelCreationChain}\n                    className={classes.funcButton}>Modellerstellung\n                neustarten</Button>\n\n            <TableContainer component={Paper} className={classes.table}>\n                <Table aria-label=\"simple table\">\n                    <TableHead>\n                        <TableRow>\n                            <TableCell>Dessert (100g serving)</TableCell>\n                            <TableCell align=\"right\">Value</TableCell>\n                        </TableRow>\n                    </TableHead>\n                    <TableBody>\n                        {rows.map((row) => (\n                            <TableRow key={row.name}>\n                                <TableCell component=\"th\" scope=\"row\">\n                                    {row.name}\n                                </TableCell>\n                                <TableCell align=\"right\">{row.value}</TableCell>\n                            </TableRow>\n                        ))}\n                    </TableBody>\n                </Table>\n            </TableContainer>\n        </div>\n    </div>;\n}\n\nexport default PredictionPage;\n","export default __webpack_public_path__ + \"static/media/small_batch32.392cf0c8.png\";","export default __webpack_public_path__ + \"static/media/small_deep.c0187fa5.png\";","export default __webpack_public_path__ + \"static/media/loss_overfitting.0c1dcbb9.png\";","export default __webpack_public_path__ + \"static/media/bias_variance.d3639136.jpeg\";","export default __webpack_public_path__ + \"static/media/learningRate_small.60e9963c.png\";","export default __webpack_public_path__ + \"static/media/learningRate_normal.07c7452d.png\";","export default __webpack_public_path__ + \"static/media/learningRate_high.c408a93a.png\";","import React from \"react\";\nimport {makeStyles} from '@material-ui/core/styles';\nimport AppBar from '@material-ui/core/AppBar';\nimport Toolbar from '@material-ui/core/Toolbar';\nimport Typography from '@material-ui/core/Typography';\nimport Button from '@material-ui/core/Button';\nimport {Link} from \"react-router-dom\";\n\nimport small_32 from \"../images/small_batch32.png\"\nimport small_deep from \"../images/small_deep.png\"\nimport loss_overfitting from \"../images/loss_overfitting.png\"\nimport bias_variance from \"../images/bias_variance.jpeg\"\n\nimport learningRate_small from \"../images/learningRate_small.png\"\nimport learningRate_normal from \"../images/learningRate_normal.png\"\nimport learningRate_high from \"../images/learningRate_high.png\"\n\nconst useStyles = makeStyles((theme) => ({\n    root: {\n        flexGrow: 1,\n    },\n    contentDiv: {\n        marginRight: 50,\n        marginLeft: 50\n    },\n    title: {\n        flexGrow: 1,\n        color: \"black\"\n    },\n    AppBar: {\n        background : \"#607d8b\"\n    },\n    LoginButton: {\n        marginRight: 50,\n        color: \"white\"\n    },\n    horizontalImages: {\n        display: \"flex\",\n        flexDirection: \"row\"\n    }\n}));\n\nconst DocumentationPage = () => {\n    const classes = useStyles();\n\n    return (\n        <div className={classes.root}>\n            <AppBar position=\"static\" className={classes.AppBar}>\n                <Toolbar>\n\n                    <Typography variant=\"h6\" className={classes.title}>\n                        Deep Learning - André Schwarz\n                    </Typography>\n                    <Link to=\"/\">\n                        <Button color=\"primary\" className={classes.LoginButton}>Aufgabe 3 - Vorhersage</Button>\n                    </Link>\n                </Toolbar>\n            </AppBar>\n\n            <div className={classes.contentDiv}>\n                <h1>Herangehensweise, Dokumentation und Quellen</h1>\n\n                <h2>Tech Stack</h2>\n\n                <h4>React js</h4>\n                Diese Anwendung wurde mit React erstellt. Dabei übernimmt React die Entwicklung des User Interfaces sowie der Navigation zwischen den\n                beiden Seiten (React Router).\n\n                <h4>Tensorflow js</h4>\n                Tensorflow ist ein Framework, das die Erstellung von Neuronalen Netzen ermöglicht. In diesem Fall wurde die Version für Javascript\n                verwendet.\n                <h4>tensorflow/tfjs-vis</h4>\n                tfjs-vis ist eine Bibliothek von Tensorflow die es erlaubt die erstellten Modelle zu visualisieren und das Training zu überwachen.\n                <h4>Material UI</h4>\n                Material UI wurde verwendet um die Standard UI Komponenten zu benutzen.\n\n                <h2>Daten</h2>\n                <p>\n                    Für das Training der Netze wurden Daten generiert. Dazu wurde die gegebene Formel\n                    iterativ mit gleichmäßig verteilten Daten berechnet.\n                    Insgesamt wurden drei verschiedene Datensätze erzeugt, die alle den Zahlenbereich von 0 bis 50\n                    abdecken.\n\n                    Der Unterschied liegt in der unterschiedlichen Stepsize von 0.01, 0.1 und 1.\n                    Dementsprechend beinhalten die Datensätze 5000, 500 und 50 Einträge.\n\n                </p>\n\n                <h2>Over- und Underfitting</h2>\n                <p>\n                    Normalerweise ist es bei der Modellerstellung immer das Ziel ein Over-, sowie Underfitting zu\n                    vermeiden.\n                    Für einen Versuch wurde dieses Verhalten jedoch provoziert.\n                </p>\n                <h3>Underfitting</h3>\n                <p>\n                    Um ein Underfitting zu erzeugen, wurde der kleine Datensatz zusammen mit einem zu tiefem Netz\n                    trainiert.\n                    Insgesamt wurden 8 hidden layer erzeugt. Das Ergebnis ist ein Modell, das dem Datensatz kaum folgen\n                    kann. Die Ergebnisse sehen reproduzierbar wie folgt aus:<br/>\n                    <img src={small_deep}/><br/>\n                    Die Vorhersage ändert sich so gut wie nicht und beschreibt eine Gerade die dem Datensatz kaum folgt.\n                </p>\n                <h3>Overfitting</h3>\n                <p>\n                    Bei der Verwenung des großen Datensatzes zusammen mit einem sehr kleinen Netz (lediglich Ein- und\n                    Ausgabeschicht) kommt es zu einem Overfitting.\n                    Der Fehler der beim Training entsteht fällt sehr schnell ab und bleibt nach einigen wenigen Epochs\n                    ohne große Änderungen.\n                    Das Modell hat die Trainingsdaten auswendig gelernt. <br/>\n                    <img src={loss_overfitting}/>\n                </p>\n\n\n                <h3>Bias und Varianz</h3>\n\n                Es gibt Modelle, die zu vereinfacht sind und wichtige Beziehungen in den Trainingsdaten ignorieren, die\n                ihre Vorhersagen hätten verbessern können.\n                Bei solchen Modellen spricht man von einem hohen Bias. Wenn ein Modell einen hohen Bias hat, sind die\n                Vorhersagen konsistent falsch,\n                im besten Fall nur für bestimmte Bereiche der Daten und nicht für den gesamten Bereich.\n                Wenn wie in diesem Beispiel versucht, eine Linie an ein Diagramm anzupassen bei dem die Daten einem\n                kurvenlinearen Muster folgen,\n                ist es relativ vorhersehbar, dass sich das Modell nicht gut an die Daten anpassen kann.\n                In einigen Teilen des Diagramms wird die Linie unter die Kurve fallen und in anderen Teilen wird sie\n                über der Kurve liegen, wobei sie versucht, dem Verlauf einer Kurve zu folgen.\n\n                Es handelt sich also um Vorhersagen, die konstant falsch sind. Bei Modellen mit einem hohen Bias spricht\n                man von einer Unteranpassung [an die Trainingsdaten],\n                und daher ist der Vorhersagefehler sowohl bei den Trainingsdaten als auch bei den Testdaten hoch.\n                Einige Modelle sind zu komplex, und bei der Suche nach wichtigen Beziehungen zwischen den Variablen\n                werden zufällig auch bestimmte Beziehungen erfasst,\n                die sich nur als Ergebnis von Rauschen herausstellen. Mit anderen Worten, das Modell berücksichtigt\n                bestimmte \"Ausreißer\" in den Trainingsdaten,\n                die sich nicht auf die Testdaten verallgemeinern lassen. In einem solchen Fall liegen die Vorhersagen\n                des Modells wieder einmal daneben,\n                aber hier ist der wichtige Teil: Sie liegen nicht konstant daneben. Bei kleinen Änderungen der Daten,\n                können sehr unterschiedliche Vorhersagen gemacht werden.\n                Das Modell ist also zu empfindlich und reagiert übermäßig auf die Veränderung der Daten. Bei Modellen\n                mit hoher Varianz spricht man von einer\n                Überanpassung [an die Trainingsdaten], und daher ist ihr Vorhersagefehler bei den Trainingsdaten\n                trügerisch niedrig, bei den Testdaten aber hoch,\n                daher die fehlende Generalisierung.\n\n                <br/>\n                Der Zusammenhang ist in folgendem Bild sehr gut ersichtlich :\n\n                <img src={bias_variance}/>\n                <br/>https://towardsdatascience.com/bias-and-variance-but-what-are-they-really-ac539817e171 <br/> <br/>\n\n                Das Ziel ist dementsprechend das Modell auf die Modell Komplexität anzupassen. Ein unglaubwürdig\n                geringer Fehler beim Training weist auf ein Problem hin und nicht auf ein generell arbeitendes Netz.\n\n                <h2>Aktivierungsfunktionen</h2>\n                <h2>Lernrate und Optimizer</h2>\n\n                Die Lernrate ist ein sehr wichtiger Hyperparameter bei der Modellerstellung. Dieser steuert wie stark\n                das Modell auf den geschätzten Fehler\n                nach der Aktualisierung der Gewichte reagieren soll.\n                Eine zu kleine Lernrate führt zu einem sehr langen Training, ein zu großer Wert zu einem zu schnellen\n                und instabilen Lernen,\n                wodurch der Lernprozess sehr schnell zum erliegen kommen kann. Dieses Verhalten ist übergreifend über\n                alle Optimizer zu beobachten. Die folgenden\n                Bilder zeigen das Verhalten beispielhaft am \"Adam Optimizer\":\n\n                <div className=\"horizontalImages\">\n                    <img src={learningRate_small}/>\n                    <img src={learningRate_normal}/>\n                    <img src={learningRate_high}/>\n                </div>\n\n                <br/>\n                Bei dem ersten Beispiel wurde eine Lernrate von 0.0001 gewählt. Hier ist ein enorm hoher Fehler zu\n                erkennen,\n                der sich im Laufe des Trainings kaum verringert.\n                Das zweite Beispiel wurde mit der Standard Lernrate von 0.001 durchgeführt. Hierbei ist eine\n                Verringerung des Fehlers erkennbar,\n                der über den vollen Trainingsvorgang stetig abnimmt.\n                Das dritte Beispiel wurde mit einer Lernrate von 0.5 durchgeführt. Der Fehler nimmt schon nach dem\n                ersten Epoch stark ab und läuft\n                dann auf einem sehr geringen Level weiter. Der Lernprozess findet jetzt kaum noch statt.\n\n                <h3>Optimizer</h3>\n\n                Tensorflow JS bietet folgende Optimizer:\n                adadelta, adagrad, adam, adamax, momentum, rmsprop, sgd\n                <br/>\n                Es wurden alle sieben Optimizer getestet. Um die Ergebnisse vergleichen zu können, immer mit einer\n                Lernrate von 0.01.\n                Die Ergebnisse sind sich sehr ähnlich (die Kurven der Fehler) und hängen eher von der Lernrate ab. Es\n                lassen sich die selben\n                Effekte wie im vorigen Kapitel erzeugen.\n                Der Adam Optimizer war durchweg um einen kleinen Betrag besser (nach 20 Epochs).\n\n\n                <h2>Anzahl der Epochs/Batch sizes</h2>\n                <p>\n                    Die Anzahl der Epochs wurde zum Start auf 20 festgelegt. Um eine vergleichbare Erzeugung des Models\n                    zu gewährleisten.\n                    Jedoch war bei so gut wie allen Modellen ersichtlich, dass der Fehler ab 10 Epochs nicht mehr stark\n                    abnimmt. Dadurch ist der Einfluss als eher gering einzuschätzen.\n\n                    Die Batch Size hat einen großen Einfluss auf die Qualität des Modells. Diese muss auf die\n                    Trainingsdatengröße angepasst\n                    sein. Bei dem kleinen Datensatz mit nur 50 Einträgen, darf nur eine geringe Batch size gewählt\n                    werden.\n                    Dazu wurden mehrere Modelle mit folgenden Batch Sizes trainiert: 5, 20, 32, 64.\n                    Die Qualität war sehr schwankend. Obwohl der Datensatz vor dem Training gemischt wurde,\n                    ist es bei so kleinen Datensätzen eher wahrscheinlich das die Daten nicht wirklich zufällig\n                    durchgearbeitet werden.\n                    Je größer die Batch Size gewählt wurde, desto öfter wurde das Modell völlig falsch trainiert und der\n                    Fehler war extremer.\n                    Folgendes Beispiel für ein sehr fehlerhaftes Modell wurde mit dem kleinen Datensatz und einer Batch\n                    Size von 32 erzeugt.\n                </p>\n                <img src={small_32} alt={small_32}/>\n\n            </div>\n        </div>\n    );\n};\n\nexport default DocumentationPage;","import React from \"react\";\n\nimport {\n    BrowserRouter as Router,\n    Route\n} from \"react-router-dom\";\nimport PredictionPage from \"./pages/prediction\";\nimport DocumentationPage from \"./pages/documentation\";\n\nfunction App() {\n\n    return <div className=\"App\">\n        <Router>\n            <Route exact path=\"/\" component={PredictionPage}/>\n            <Route exact path=\"/documentation\" component={DocumentationPage}/>\n        </Router>\n    </div>;\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}